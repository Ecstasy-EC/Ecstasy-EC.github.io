pub_date	title	venue	excerpt	citation	url_slug	paper_url	bibtex	video	img
2023-03-15	A Self-rotating, Single-actuated UAV with Extended Sensor Field of View for Autonomous Navigation	Science Robotics	Uncrewed aerial vehicles (UAVs) rely heavily on visual sensors to perceive obstacles and explore environments. Current UAVs are limited in both perception capability and task efficiency because of a small sensor field of view (FoV). One solution could be to leverage self-rotation in UAVs to extend the sensor FoV without consuming extra power. This natural mechanism, induced by the counter-torque of the UAV motor, has rarely been exploited by existing autonomous UAVs because of the difficulties in design and control due to highly coupled and nonlinear dynamics and the challenges in navigation brought by the high-rate self-rotation. Here, we present powered-flying ultra-underactuated LiDAR (light detection and ranging) sensing aerial robot (PULSAR), an agile and self-rotating UAV whose three-dimensional position is fully controlled by actuating only one motor to obtain the required thrust and moment. The use of a single actuator effectively reduces the energy loss in powered flights. Consequently, PULSAR consumes 26.7% less power than the benchmarked quadrotor with the same total propeller disk area and avionic payloads while retaining a good level of agility. Augmented by an onboard LiDAR sensor, PULSAR can perform autonomous navigation in unknown environments and detect both static and dynamic obstacles in panoramic views without any external instruments. We report the experiments of PULSAR in environment exploration and multidirectional dynamic obstacle avoidance with the extended FoV via self-rotation, which could lead to increased perception capability, task efficiency, and flight safety.	Chen, N., Kong, F., Xu, W., Cai, Y., Li, H., He, D., ... & Zhang, F. (2023). A self-rotating, single-actuated UAV with extended sensor field of view for autonomous navigation. <i>Science Robotics</i>, 8(76), eade4538.	PULSAR-SR	https://www.science.org/doi/10.1126/scirobotics.ade4538	https://scholar.googleusercontent.com/scholar.bib?q=info:-n8qyHFAUjIJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o2XGKc:AGlGAw8AAAAAZHCSAKfg37WQDSeGi80O4_wIfaQ&scisig=AGlGAw8AAAAAZHCSADpcfSvOAhBjYT4dEJOJ1BE&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=lrEJnJrRJsQ	-
2022-01-31	FAST-LIO2: Fast Direct Lidar-Inertial Odometry	IEEE Transactions on Robotics (TRO)	This article presents FAST-LIO2: a fast, robust, and versatile LiDAR-inertial odometry framework. Building on a highly efficient tightly coupled iterated Kalman filter, FAST-LIO2 has two key novelties that allow fast, robust, and accurate LiDAR navigation (and mapping). The first one is directly registering raw points to the map (and subsequently update the map, i.e., mapping) without extracting features. This enables the exploitation of subtle features in the environment and, hence, increases the accuracy. The elimination of a hand-engineered feature extraction module also makes it naturally adaptable to emerging LiDARs of different scanning patterns; the second main novelty is maintaining a map by an incremental k-dimensional (k-d) tree data structure, incremental k-d tree (ikd-Tree), that enables incremental updates (i.e., point insertion and delete) and dynamic rebalancing. Compared with existing dynamic data structures (octree, R?- tree, and nanoflann k-d tree), ikd-Tree achieves superior overall performance while naturally supports downsampling on the tree. We conduct an exhaustive benchmark comparison in 19 sequences from a variety of open LiDAR datasets. FAST-LIO2 achieves consistently higher accuracy at a much lower computation load than other state-of-the-art LiDAR-inertial navigation systems.Various real-world experiments on solid-state LiDARs with small field of view are also conducted. Overall, FAST-LIO2 is computationally efficient (e.g., up to 100 Hz odometry and mapping in large outdoor environments), robust (e.g., reliable pose estimation in cluttered indoor environments with rotation up to 1000 deg/s), versatile (i.e., applicable to both multiline spinning and solid-state LiDARs, unmanned aerial vehicle (UAV) and handheld platforms, and Intel and ARM-based processors), while still achieving a higher accuracy than existing methods. Our implementation of the system FAST-LIO2 and the data structure ikd-Tree are both open-sourced on Github.	"W. Xu, Y. Cai, D. He, J. Lin and F. Zhang, ""FAST-LIO2: Fast Direct LiDAR-Inertial Odometry,"" in <i>IEEE Transactions on Robotics</i>, vol. 38, no. 4, pp. 2053-2073, Aug. 2022, doi: 10.1109/TRO.2022.3141876."	FAST-LIO2	https://ieeexplore.ieee.org/abstract/document/9697912	https://scholar.googleusercontent.com/scholar.bib?q=info:8zeOJmTTn5cJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o2UUik:AGlGAw8AAAAAZHCRSinFWegK-ZjV-NMPJMPgtdU&scisig=AGlGAw8AAAAAZHCRSkLKAn_p2MmN_VfjpKk8Hko&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=2OvjGnxszf8	-
2022-01-10	Robots' State Estimation and Observability Analysis Based on Statistical Motion Models	IEEE Transactions on Control Systems Technology (TCST)	This article presents a generic motion model to capture mobile robots' dynamic behaviors (translation and rotation). The model is based on statistical models driven by white random processes and is formulated into a full state estimation algorithm based on the error-state extended Kalman filtering framework (ESEKF). The major benefits of this method are its versatility, being applicable to different robotic systems without accurately modeling the robots' specific dynamics, and the ability to estimate the robot's (angular) acceleration, jerk, or higher order dynamic states with low delay. Mathematical analyses with numerical simulations are presented to show the properties of the statistical model-based estimation framework and reveal its connection to existing low-pass filters. Furthermore, a new paradigm is developed for robotic observability analysis by developing Lie derivatives and associated partial differentiation directly on manifolds. It is shown that this new paradigm is much simpler and more natural than existing methods based on quaternion parameterizations. It is also scalable to highdimensional systems. A novel thin set concept is introduced to characterize the unobservable subset of the system states, providing the theoretical foundation to observability analysis of robotic systems operating on manifolds and in high dimension. Finally, extensive experiments, including full state estimation and extrinsic calibration (both POS-IMU and IMU-IMU) on a quadrotor unmanned aerial vehicle (UAV), a handheld platform, and a ground vehicle, are conducted. Comparisons with existing methods show that the proposed method can effectively estimate all extrinsic parameters, the robot's translation/angular acceleration, and other state variables (e.g., position, velocity, and attitude) with high accuracy and low delay.	"W. Xu, D. He, Y. Cai and F. Zhang, Robots' State Estimation and Observability Analysis Based on Statistical Motion Models,"" in <i>IEEE Transactions on Control Systems Technology</i>, vol. 30, no. 5, pp. 2030-2045, Sept. 2022, doi: 10.1109/TCST.2021.3133080."""	TCST-Robot	https://ieeexplore.ieee.org/document/9676487	https://scholar.googleusercontent.com/scholar.bib?q=info:du0fIlL7gzMJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o21y_Y:AGlGAw8AAAAAZHCw0_ZU1F1LBr0HFwXcGrti7aI&scisig=AGlGAw8AAAAAZHCw01DaVbIlsAWE3aBw26Rh1Ws&scisf=4&ct=citation&cd=-1		-
2022-03-16	Gemini II: Design, Modeling, and Control of a Compact Yet Efficient Servoless Bi-copter	IEEE/ASME Transactions on Mechatronics (TMECH)	Unmanned aerial vehicles (UAV) are widely used in the field for tasks that require 3-D movements in space. The simultaneous demand for heavy payload capacity, long operating time, and size restriction poses a challenge to UAV design. To solve this conundrum, this article proposes a novel bi-copter with only two actuators. Unlike tandem rotor bi-copters that utilize two servomotors to achieve yaw and roll control, our novel design, the Gemini II bi-copter, controls attitude by using cyclic flapping response in hinges that connect the blades. This passive cyclic pitch-varying mechanism makes the UAV no longer depend on two heavy and expensive servo motors or swashplate to vector the thrust. This change of propulsion system not only makes the UAV mechanically simpler, more reliable, and cost-effective, but also enhances the UAV performance by mitigating the issues of backlash, nonlinearity, and nonminimum phase caused by servo motors. To the best of the authors' knowledge, the Gemini II, built entirely on off-the-shelf electronics, is the first servoless bi-copter that can precisely control its 3-D position and orientation with two actuators only. To demonstrate the flight performance and applications of the proposed novel UAV, we conduct path-following experiments along with manual poking and wind disturbances tests.	"Y. Qin, N. Chen, Y. Cai, W. Xu and F. Zhang, Gemini II: Design, Modeling, and Control of a Compact Yet Efficient Servoless Bi-copter,"" in <i>IEEE/ASME Transactions on Mechatronics</i>, vol. 27, no. 6, pp. 4304-4315, Dec. 2022, doi: 10.1109/TMECH.2022.3153587."""	GeminiII	https://ieeexplore.ieee.org/abstract/document/9736334	https://scholar.googleusercontent.com/scholar.bib?q=info:ZFIUUJSfO-sJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o238Vc:AGlGAw8AAAAAZHCy6Vd72CglpF4sPcw3P8TChO4&scisig=AGlGAw8AAAAAZHCy6edAXrfWRWz2Qmj8ilSDCzw&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=qGhQbPtp7Sw	-
2021-08-04	Avoiding Dynamic Small Obstacles with Onboard Sensing and Computation on Aerial Robots	IEEE Robotics and Automation Letters (RAL)	In practical applications, autonomous quadrotors are still facing significant challenges, such as the detection and avoidance of very small and even dynamic obstacles (e.g., tree branches, power lines). In this paper, we propose a compact, integrated, and fully autonomous quadrotor system, which can fly safely in cluttered environments while avoiding dynamic small obstacles. Our quadrotor platform is equipped with a forward-looking threedimensional (3D) light detection and ranging (lidar) sensor to perceive the environment and an onboard embedded computer to perform all the estimation, mapping, and planning tasks. Specifically, the computer estimates the current pose of theUAV, maintains a local map (time-accumulated point clouds KD-Trees), and computes a safe trajectory using kinodynamic A* search to the goal point. The whole perception and planning system can run onboard at 50 Hz. Various indoor and outdoor experiments show that the system can avoid dynamic small obstacles (down to 9mmdiameter bar) while flying at 2 m/s in cluttered environments. High-speed experiments are also carried out, with amaximum speed of 5.5 m/s. Our codes are open-sourced on Github.	"F. Kong, W. Xu, Y. Cai and F. Zhang, Avoiding Dynamic Small Obstacles With Onboard Sensing and Computation on Aerial Robots,"" in <i>IEEE Robotics and Automation Letters</i>, vol. 6, no. 4, pp. 7869-7876, Oct. 2021, doi: 10.1109/LRA.2021.3101877."""	Avoiding	https://ieeexplore.ieee.org/abstract/document/9507303	https://scholar.googleusercontent.com/scholar.bib?q=info:KCq1X3ZjdlAJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o2xybo:AGlGAw8AAAAAZHC00boXmMxZZMnbLNbrVtxFtWk&scisig=AGlGAw8AAAAAZHC00SAO_EzBoI5bjAoUekiHhls&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=mjtmpEYwQsI	-
2023-04-03	MARSIM: A light-weight point-realistic simulator for LiDAR-based UAVs	IEEE Robotics and Automation Letters (RAL)	The emergence of LiDAR sensors have brought new opportunities for autonomous unmanned aerial vehicles (UAVs) by advancing navigation safety and computation efficiency. Yet the successful developments of LiDAR-based UAVs must rely on extensive simulations. Existing simulators can hardly perform simulations of real-world environments due to the requirements of dense mesh maps that are difficult to obtain. Therefore, we develop a point-realistic simulator of real-world scenes for LiDAR-based UAVs. The key idea is the underlying point rendering method, where we construct a depth image directly from the point cloud map and interpolate it to obtain realistic LiDAR point measurements. Our developed simulator is able to run on a light-weight computing platform and supports the simulation of LiDARs with different resolution and scanning patterns, dynamic obstacles, and multi-UAV systems. Developed in the ROS framework, the simulator can easily communicate with other key modules of an autonomous robot, such as perception, state estimation, planning, and control. Finally, the simulator provides 10 high-resolution point cloud maps of various real-world environments, including forests of different densities, historic building, office, parking garage, and various complex indoor environments. Evaluation results show that the developed simulator achieves superior performance in terms of time and memory consumption against Gazebo and that the simulated UAV flights highly match the actual one in real-world environments. We believe such a point-realistic and light-weight simulator is crucial to bridge the gap between UAV simulation and experiments and will significantly facilitate the research of LiDAR-based autonomous UAVs in the future.	Kong, F., Liu, X., Tang, B., Lin, J., Ren, Y., Cai, Y., ... & Zhang, F. (2023). MARSIM: A light-weight point-realistic simulator for LiDAR-based UAVs. <i>IEEE Robotics and Automation Letters</i>, 8(5), 2954-2961.	MARSIM	https://ieeexplore.ieee.org/abstract/document/10091117/	https://scholar.googleusercontent.com/scholar.bib?q=info:WgFX3rhoj9wJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o2w1zc:AGlGAw8AAAAAZHC1zzdaY1uIGyAIimicqX8fWQE&scisig=AGlGAw8AAAAAZHC1z58CitqKXD3fEkZaAmzV6_w&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=hiRtcq-5lN0	-
2021-02-22	ikd-Tree: An Incremental KD Tree for Robotic Applications	Preprint	This paper proposes an efficient data structure, ikd-Tree, for dynamic space partition. The ikd-Tree incrementally updates a k-d tree with new coming points only, leading to much lower computation time than existing static k-d trees. Besides point-wise operations, the ikd-Tree supports several features such as box-wise operations and down-sampling that are practically useful in robotic applications. In parallel to the incremental operations (i.e., insert, re-insert, and delete), ikd-Tree actively monitors the tree structure and partially rebalances the tree, which enables efficient nearest point search in later stages. The ikd-Tree is carefully engineered and supports multi-thread parallel computing to maximize the overall efficiency. We validate the ikd-Tree in both theory and practical experiments. On theory level, a complete time complexity analysis is presented to prove the high efficiency. On experiment level, the ikd-Tree is tested on both randomized datasets and real-world LiDAR point data in LiDAR-inertial odometry and mapping application. In all tests, ikd-Tree consumes only 4% of the running time in a static k-d tree.	Cai, Y., Xu, W., & Zhang, F. (2021). ikd-tree: An incremental kd tree for robotic applications. <i>arXiv preprint</i> arXiv:2102.10808.	ikdTree	https://arxiv.org/pdf/2102.10808.pdf	https://scholar.googleusercontent.com/scholar.bib?q=info:GKtGHIuR5IYJ:scholar.google.com/&output=citation&scisdr=Cm3pnLgeELeo6o2zqeQ:AGlGAw8AAAAAZHC2seQn7FhczrGc5gy9ZFNO6dk&scisig=AGlGAw8AAAAAZHC2sfgCS3JErzB8mfMMDl-Vq1s&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=ueOunk03zxA	-
2023-09-19	Trajectory Generation and Tracking Control for Aggressive Tail-Sitter Flights	International Journal of Robotics Research	"We address the theoretical and practical problems related to the trajectory generation and tracking control of tail-sitter UAVs. Theoretically, we focus on the differential flatness property with full exploitation of actual UAV aerodynamic models, which lays a foundation for generating dynamically feasible trajectory and achieving high-performance tracking control. We have found that a tail-sitter is differentially flat with accurate (not simplified) aerodynamic models within the entire flight envelope, by specifying coordinate flight condition and choosing the vehicle position as the flat output. This fundamental property allows us to fully exploit the high-fidelity aerodynamic models in the trajectory planning and tracking control to achieve accurate tail-sitter flights. Particularly, an optimization-based trajectory planner for tail-sitters is proposed to design high-quality, smooth trajectories with consideration of kinodynamic constraints, singularity-free constraints, actuator saturation, and obstacle avoidance. The planned trajectory of flat output is transformed to state trajectory in real-time with optional consideration of wind gust. To track the state trajectory, a global, singularity-free, and minimally-parameterized on-manifold MPC is developed, which fully leverages the accurate aerodynamic model to achieve high-accuracy trajectory tracking in the whole flight envelope. The proposed algorithms are implemented on our quadrotor tail-sitter prototype, ``Hong Hu"", and their effectiveness are demonstrated through extensive real-world experiments in both indoor and outdoor field tests, including agile SE(3) flight through consecutive narrow windows (size narrower than the UAV wingspan) with speed up to $10$m/s, typical tail-sitter maneuvers (hovering , transition, level flight and loiter) with speed up to $20$m/s, and extremely aggressive aerobatic maneuvers (Wingover, Loop, Vertical Eight and Cuban Eight) with acceleration up to 2.5g."	Lu, G., Cai, Y., Chen, N., Kong, F., Ren, Y., & Zhang, F. (2022). Trajectory Generation and Tracking Control for Aggressive Tail-Sitter Flights. arXiv preprint arXiv:2212.11552.	tailsitter	https://doi.org/10.1177/02783649231207655	https://scholar.googleusercontent.com/scholar.bib?q=info:Qb3Iodb5HXMJ:scholar.google.com/&output=citation&scisdr=ClEjYb5MELeo6_R1yzM:AFWwaeYAAAAAZQlw0zNsfxwMWf0I7ZrhF5j4U2o&scisig=AFWwaeYAAAAAZQlw03CJwIGKeYpGN29shWIHkpQ&scisf=4&ct=citation&cd=-1&hl=zh-CN	https://www.youtube.com/watch?v=2x_bLbVuyrk	https://img.youtube.com/vi/2x_bLbVuyrk/hqdefault.jpg
2023-09-19	Immesh: An immediate lidar localization and meshing framework	IEEE Transactions on Robotics (TRO)	In this paper, we propose a novel LiDAR(-inertial) odometry and mapping framework to achieve the goal of simultaneous localization and meshing in real-time. This proposed framework termed ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module first utilizes the preprocessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster. The primary contribution of this work is the meshing module, which represents a scene by an efficient voxel structure, performs fast finding of voxels observed by new scans, and incrementally reconstructs triangle facets in each voxel. This voxel-wise meshing operation is delicately designed for the purpose of efficiency; it first performs a dimension reduction by projecting 3D points to a 2D local plane contained in the voxel, and then executes the meshing operation with pull, commit and push steps for incremental reconstruction of triangle facets. To the best of our knowledge, this is the first work in literature that can reconstruct online the triangle mesh of large-scale scenes, just relying on a standard CPU without GPU acceleration. To share our findings and make contributions to the community, we make our code publicly available on our [GitHub](https://github.com/hku-mars/ImMesh).	Lin, J., Yuan, C., Cai, Y., Li, H., Zou, Y., Hong, X., & Zhang, F. (2023). Immesh: An immediate lidar localization and meshing framework. arXiv preprint arXiv:2301.05206.	ImMesh	https://ieeexplore.ieee.org/document/10304337	https://scholar.googleusercontent.com/scholar.bib?q=info:jbOX-E48bg0J:scholar.google.com/&output=citation&scisdr=ClEjYb5MELeo67TQQ0E:AFWwaeYAAAAAZUnVW0H730ZYJyTtAcMIUAyqFrM&scisig=AFWwaeYAAAAAZUnVW4YYy_fJoieY-BPmKnF5uZY&scisf=4&ct=citation&cd=-1&hl=zh-CN	https://www.youtube.com/watch?v=pzT2fMwz428	-
2023-09-30	Occupancy Grid Mapping without Ray-Casting for High-resolution Sensors	IEEE Transactions on Robotics (TRO)	Occupancy mapping is a fundamental component of robotic systems to reason about the unknown and known regions of the environment. This article presents an efficient occupancy mapping framework for high-resolution LiDAR sensors, termed D-Map. The framework introduces three main novelties to address the computational efficiency challenges of occupancy mapping. Firstly, we use a depth image to determine the occupancy state of regions instead of the traditional ray-casting method. Secondly, we introduce an efficient on-tree update strategy on a tree-based map structure. These two techniques avoid redundant visits to small cells, significantly reducing the number of cells to be updated. Thirdly, we remove known cells from the map at each update by leveraging the low false alarm rate of LiDAR sensors. This approach not only enhances our framework's update efficiency by reducing map size but also endows it with an interesting decremental property, which we have named D-Map. To support our design, we provide theoretical analyses of the accuracy of the depth image projection and time complexity of occupancy updates. Furthermore, we conduct extensive benchmark experiments on various LiDAR sensors in both public and private datasets. Our framework demonstrates superior efficiency in comparison with other state-of-the-art methods while maintaining comparable mapping accuracy and high memory efficiency. We demonstrate two real-world applications of D-Map for real-time occupancy mapping on a handle device and an aerial platform carrying a high-resolution LiDAR. In addition, we open-source the implementation of D-Map on GitHub to benefit society: [GitHub](https://github.com/hku-mars/D-Map)	"Y. Cai, F. Kong, Y. Ren, F. Zhu, J. Lin and F. Zhang, ""Occupancy Grid Mapping Without Ray-Casting for High-Resolution LiDAR Sensors,"" in  <i>IEEE Transactions on Robotics</i>, doi: 10.1109/TRO.2023.3323936."	DMap	https://ieeexplore.ieee.org/document/10286126	https://scholar.googleusercontent.com/scholar.bib?q=info:br2xAwjho20J:scholar.google.com/&output=citation&scisdr=ClEjYb5MELeo67TQmQc:AFWwaeYAAAAAZUnVgQdB-csinkQeaVjWz0S10l8&scisig=AFWwaeYAAAAAZUnVgeP5OSNOe7A6lQNU9RA2M3Y&scisf=4&ct=citation&cd=-1&hl=zh-CN	https://www.youtube.com/watch?v=m5QQPbkYYnA	-
2023-12-26	MARS-LVIG dataset: A multi-sensor aerial robots SLAM dataset for LiDAR-visual-inertial-GNSS fusion	International Journal of Robotics Research	In recent years, advancements in Light Detection and Ranging (LiDAR) technology have made 3D LiDAR sensors more compact, lightweight, and affordable. This progress has spurred interest in integrating LiDAR with sensors such as Inertial Measurement Units (IMUs) and cameras for Simultaneous Localization and Mapping (SLAM) research. Public datasets covering different scenarios, platforms, and viewpoints are crucial for multi-sensor fusion SLAM studies, yet most focus on handheld or vehicle-mounted devices with front or 360-degree views. Data from aerial vehicles with downward-looking views is scarce, existing relevant datasets usually feature low altitudes and are mostly limited to small campus environments. To fill this gap, we introduce the Multi-sensor Aerial Robots SLAM dataset (MARS-LVIG dataset), providing unique aerial downward-looking LiDAR-Visual-Inertial-GNSS data with viewpoints from altitudes between 80?m and 130?m. The dataset not only offers new aspects to test and evaluate existing SLAM algorithms, but also brings new challenges which can facilitate researches and developments of more advanced SLAM algorithms. The MARS-LVIG dataset contains 21 sequences, acquired across diversified large-area environments including an aero-model airfield, an island, a rural town, and a valley. Within these sequences, the UAV has speeds varying from 3m/s to 12m/s, a scanning area reaching up to 577,000?m2, and the max path length of 7.148?km in a single flight. This dataset encapsulates data collected by a lightweight, hardware-synchronized sensor package that includes a solid-state 3D LiDAR, a global-shutter RGB camera, IMUs, and a raw message receiver of the Global Navigation Satellite System (GNSS). For algorithm evaluation, this dataset releases ground truth of both localization and mapping, which are acquired by on-board Real-time Kinematic (RTK) and DJI L1 (post-processed by its supporting software DJI Terra), respectively. The dataset can be downloaded from: https://mars.hku.hk/dataset.html.	Haotian Li and Yuying Zou and Nan Chen and Jiarong Lin and Xiyuan Liu and Wei Xu and Chunran Zheng and Rundong Li and Dongjiao He and Fanze Kong and Yixi Cai and Zheng Liu and Shunbo Zhou and Kaiwen Xue and Fu Zhang.(2024). MARS-LVIG dataset: A multi-sensor aerial robots SLAM dataset for LiDAR-visual-inertial-GNSS fusion. The International Journal of Robotics Research.	MARS-LVIG	https://journals.sagepub.com/doi/10.1177/02783649241227968	https://journals.sagepub.com/doi/10.1177/02783649241227968	https://mars.hku.hk/images/dataset/265.mp4	https://mars.hku.hk/images/dataset/airport_gif~1.gif
2024-06-30	ROG-Map: An Efficient Robocentric Occupancy Grid Map for Large-scene and High-resolution LiDAR-based Motion Planning	IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)	Recent advances in LiDAR technology have opened up new possibilities for robotic navigation. Given the widespread use of occupancy grid maps (OGMs) in robotic motion planning, this paper aims to address the challenges of integrating LiDAR with OGMs. To this end, we propose ROG-Map, a uniform grid-based OGM that maintains a local map moving along with the robot to enable efficient map operation and reduce memory costs for large-scene autonomous flight. Moreover, we present a novel incremental obstacle inflation method that significantly reduces the computational cost of inflation. The proposed method outperforms state-of-the-art (SOTA) methods on various public datasets. To demonstrate the effectiveness and efficiency of ROG-Map, we integrate it into a complete quadrotor system and perform autonomous flights against both small obstacles and large-scale scenes. During real-world flight tests with a 0.05 m resolution local map and 30 m×30 m×6 m local map size, ROG-Map takes only 29.8 % of frame time on average to update the map at a frame rate of 50 Hz (i.e., 5.96 ms in 20 ms), including 0.33 % (i.e., 0.66 ms) to perform obstacle inflation, demonstrating outstanding real-world performance. We release ROG-Map as an open-source ROS package1 to promote the development of LiDAR-based motion planning.	Ren, Y., Cai, Y., Zhu, F., Liang, S., & Zhang, F. (2023). ROG-map: An efficient robocentric occupancy grid map for large-scene and high-resolution LiDAR-based motion planning. In 2024, International Conference on Intelligent Robots and Systems (IROS). IEEE.	ROG-Map	https://arxiv.org/abs/2302.14819	https://scholar.googleusercontent.com/scholar.bib?q=info:gp6JrjDfbAAJ:scholar.google.com/&output=citation&scisdr=ClEQedAGEPCUr_MtYwQ:AFWwaeYAAAAAZ0crewTOd1yqDGcbL86aUTnahKU&scisig=AFWwaeYAAAAAZ0cre2MyvkoZWIr0dttgq-CPhII&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=eDkwGXCea7w	-
2024-11-03	Large-scale Multi-session Point-cloud Map Merging	IEEE Robotics and Automation Letters (RAL)	This paper introduces LAMM, an open-source framework for large-scale multi-session 3D LiDAR point cloud map merging. LAMM can automatically integrate sub-maps from multiple agents carrying LiDARs with different scanning patterns, facilitating place feature extraction, data association, and global optimization in various environments. Our framework incorporates two key novelties that enable robust, accurate, large-scale map merging. {The first novelty is a temporal bidirectional filtering mechanism that removes dynamic objects from 3D LiDAR point cloud data. This eliminates the effect of dynamic objects on the 3D map model, providing higher-quality map merging results. }The second novelty is a robust and efficient outlier removal algorithm for detected loop closures. This algorithm ensures a high recall rate and a low false alarm rate in position retrieval, significantly reducing outliers in repetitive environments during large-scale merging. We evaluate our framework using various datasets, including KITTI, HeLiPR, WildPlaces, and a self-collected colored point cloud dataset. The results demonstrate that our proposed framework can accurately merge maps captured by different types of LiDARs and data acquisition devices across diverse scenarios.	"Wei, H., Li, R., Cai, Y., Yuan, C., Ren, Y., Zou, Z., Wu, H., Zheng, C., Zhou, S., Xue, K., Zhang, F. (2023) ""Large-scale Multi-session Point-cloud Map Merging,"" in IEEE Robotics and Automation Letters, doi: 10.1109/LRA.2024.3504317"	LAMM	https://ieeexplore.ieee.org/document/10759717	https://scholar.googleusercontent.com/scholar.bib?q=info:ZSjqOjeqa-MJ:scholar.google.com/&output=citation&scisdr=ClHG5djHEJnQqscYNT0:AFWwaeYAAAAAZ34eLT3YER81yr-IzTMgFQTVsjo&scisig=AFWwaeYAAAAAZ34eLRJbGt7KQJUsSmo4gIX30vA&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=X2WSILJe-Ew	-
2024-11-27	Swarm-LIO2: Decentralized Efficient LiDAR-inertial Odometry for UAV Swarms	IEEE Transactions on Robotics (TRO)	Aerial swarm systems possess immense potential in various aspects, such as cooperative exploration, target tracking, search and rescue. Efficient, accurate self and mutual state estimation are the critical preconditions for completing these swarm tasks, which remain challenging research topics. This paper proposes Swarm-LIO2: a fully decentralized, plug-and-play, computationally efficient, and bandwidth-efficient LiDAR-inertial odometry for aerial swarm systems. Swarm-LIO2 uses a decentralized, plug-and-play network as the communication infrastructure. Only bandwidth-efficient and low-dimensional information is exchanged, including identity, ego-state, mutual observation measurements, and global extrinsic transformations. To support the plug-and-play of new teammate participants, Swarm-LIO2 detects potential teammate UAVs and initializes the temporal offset and global extrinsic transformation all automatically. To enhance the initialization efficiency, novel reflectivity-based UAV detection, trajectory matching, and factor graph optimization methods are proposed. For state estimation, Swarm-LIO2 fuses LiDAR, IMU, and mutual observation measurements within an efficient ESIKF framework, with careful compensation of temporal delay and modeling of measurements to enhance the accuracy and consistency. Moreover, the proposed ESIKF framework leverages the global extrinsic for ego-state estimation in case of LiDAR degeneration or refines the global extrinsic along with the ego-state estimation otherwise. To enhance the scalability, Swarm-LIO2 introduces a novel marginalization method in the ESIKF, which prevents the growth of computational time with swarm size. Extensive simulation and real-world experiments demonstrate the broad adaptability to large-scale aerial swarm systems and complicated scenarios, including GPS-denied scenes, degenerated scenes for cameras or LiDARs. The experimental results showcase the centimeter-level localization accuracy which outperforms other state-of-the-art LiDAR-inertial odometry for a single UAV system. Furthermore, diverse applications demonstrate the potential of Swarm-LIO2 to serve as reliable infrastructure for various aerial swarm missions. In addition, we open-source all the system designs on GitHub to benefit society: https://github.com/hku-mars/Swarm-LIO2.	Zhu, F., Ren, Y., Yin, L., Kong, F., Liu, Q., Xue, R., Liu, W., Cai, Y., Lu, G., Li, H. & Zhang, F. (2024). Swarm-LIO2: Decentralized, Efficient LiDAR-inertial Odometry for UAV Swarms. in <i>IEEE Transactions on Robotics</i>.	SWARM-LIO2	https://ieeexplore.ieee.org/abstract/document/10816004	https://scholar.googleusercontent.com/scholar.bib?q=info:gxa7PCDd3eAJ:scholar.google.com/&output=citation&scisdr=ClHG5djHEJnQqscYn0E:AFWwaeYAAAAAZ34eh0GL6pXklXYN-WA4DPvCzgc&scisig=AFWwaeYAAAAAZ34eh0WQylXC1KLLog5Oy3rE3Yg&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=Q7cJ9iRhlrY	-
2024-12-19	Autonomous Tail-Sitter Flights in Unknown Environments	IEEE Transactions on Robotics (TRO)	"Trajectory generation for fully autonomous flights of tail-sitter unmanned aerial vehicles (UAVs) presents substantial challenges due to their highly nonlinear aerodynamics. In this paper, we introduce, to the best of our knowledge, the world's first fully autonomous tail-sitter UAV capable of high-speed navigation in unknown, cluttered environments. The UAV autonomy is enabled by cutting-edge technologies including LiDAR-based sensing, differential-flatness-based trajectory planning and control with purely onboard computation. In particular, we propose an optimization-based tail-sitter trajectory planning framework that generates high-speed, collision-free, and dynamically-feasible trajectories. To efficiently and reliably solve this nonlinear, constrained 	extcolor{black}{problem}, we develop an efficient feasibility-assured solver, EFOPT, tailored for the online planning of tail-sitter UAVs. We conduct extensive simulation studies to benchmark EFOPT's superiority in planning tasks against conventional NLP solvers. We also demonstrate exhaustive experiments of aggressive autonomous flights with speeds up to 15m/s in various real-world environments, including indoor laboratories, underground parking lots, and outdoor parks."	Lu, G., Ren, Y., Zhu, F., Li, H., Xue, R., Cai, Y., ... & Zhang, F. (2024). Autonomous Tail-Sitter Flights in Unknown Environments. in <i>IEEE Transactions on Robotics</i>.	Tailsitter-TRO	https://ieeexplore.ieee.org/document/10829730	https://scholar.googleusercontent.com/scholar.bib?q=info:51g-l06bAyQJ:scholar.google.com/&output=citation&scisdr=ClE7y4DKEJnQqt0db0w:AFWwaeYAAAAAZ2Qbd0zj95J_tNdAmCWRJlLbPKs&scisig=AFWwaeYAAAAAZ2Qbd8b8wEL1wR7UXNFXX0mGHmI&scisf=4&ct=citation&cd=-1	https://www.youtube.com/watch?v=OvqhlB2h3k8	-